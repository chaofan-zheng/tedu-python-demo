王伟超
wangweichao@tedu.cn
班级-中心-姓名

1、第四阶段课程介绍(13个工作日)
   1.1> 爬虫(7-8天左右)
        爬虫工程师
        作为Python开发工程师、数据分析工程师的附加技能
   1.2> Hadoop大数据(2-3左右)
   1.3> 数据结构(3天左右)
2、课程特点
   2.1> 综合
        第一阶段、MySQL、Redis、MongoDB、正则表达式re、多线程、多进程爬虫、HTML、JavaScript... 爬虫本身知识点
   2.2> 抽象(Hadoop、数据结构)
3、警惕
   3.1> 政府、公安机关的网站不能抓!!!
   3.2> 涉及到个人隐私数据(比如个人简历)
   遵纪守法,热爱生活

******************************************
1、爬虫分类及robots协议
   1.1> 通用网络爬虫-搜索引擎公司
        robots协议：网站会通过robots协议来告诉搜索引擎哪些页面可以抓,哪些页面不能抓,通用网络爬虫必须得遵守robots协议
   1.2> 聚焦网络爬虫
        自己写的爬虫程序,面向主题的爬虫,面向需求的爬虫
2、请求模块requests
   html = requests.get(url=url,headers={}).text
   html = requests.get(url=url,headers={}).content.decode('utf-8', 'ignore')
   响应对象resp属性
       .text    : 字符串
       .content : 字节串
       .status_code : HTTP响应码
       .url     : 返回实际数据的URL地址
3、写爬虫程序流程
   3.1> 确认数据来源：右键查看网页源代码,搜索抓取的关键字
   3.2> 响应内容中存在：分析观察URL地址的规律
   3.3> 观察页面结构,完成正则表达式
        3.3.1> 写法一：看着页面结构手写
	3.3.2> 写法二：右键-copy-copy Element-改！
4、正则解析模块使用
   4.1> 用法：r_list = re.findall(regex, html, re.S)
   4.2> 结果
        r_list : 空列表(正则错误或者响应内容不对)
        r_list : ['','',...] (正则中1个分组)
	r_list : [(),(),...] (正则中多个分组)
5、数据持久化-MySQL
   5.1> __init__()创建数据库连接对象和游标对象
       self.db = pymysql.connect(.......)
       self.cur = self.db.cursor()
   5.2> save_html()将数据处理成列表(元组)存入数据库表中
       self.cur.execute(ins, [])
       self.db.commit()
   5.3> crawl()等所有页的数据抓取完成后断开数据库
       self.cur.close()
       self.db.close()
6、数据持久化-csv
   5.1> __init__()中打开文件并初始化csv文件写入对象
        self.f = open('文件名', 'w')
	self.writer = csv.writer(self.f)
   5.2> save_html()将数据处理成列表(元组)存入csv文件中
        self.writer.writerow([])
   5.3> crawl()等所有页的数据抓取完成后关闭文件
        self.f.close()
7、数据持久化-MongoDB
   7.1> __init__()中创建连接对象、库对象和集合对象
        self.conn = pymongo.MongoClient('localhost', 27017)
	self.db = self.conn['库名']
	self.myset = self.db['集合名']
   7.2> save_html()将数据处理成字典,然后存入mongodb数据库中
        self.myset.insert_one({})
   7.3> 常用命令
        >show dbs
	>use 库名
	>show collections
	>db.集合名.count()
	>db.集合名.find().pretty()
	>db.dropDatabase()
        MongoDB是一个基于磁盘存储的非关系数据库,文档型的数据库(库 - 集合 - JSON文档)
8、增量爬虫
   8.1> 原理 ：记录爬取过的指纹,每次抓之前先比对指纹
   8.2> 实现 ：利用Redis中集合来实现,存储每个请求的指纹
   8.3> 为什么使用redis
        原因1：Redis数据库基于内存,查询比对速度特别快
	原因2：Redis中自带集合数据类型,具有无序去重的特点,非常适合存储所有请求的指纹
9、目前遇到的反爬
   9.1> 反爬：针对请求头反爬(检查请求头合法性,User-Agent)
        解决：利用headers参数,发送请求时包装请求头
   9.2> 反爬：针对User-Agent访问频率反爬
        解决：建立自己的User-Agent池,每次访问随机切换User-Agent;利用fake_useragent模块,随机切换User-Agent






写程序思路：
1、确认数据来源 - 静态
2、用正则还是xpath？？？,自己选择一个
3、写程序
4、运行程序可能出现的问题：
   问题：匹配出来空列表
   排错思路:
      思路1：确认xpath或者正则表达式
      思路2：打印响应内容,观察节点结构和页面中的区别,以响应内容为准来调整xpath或正则表达式











xpath：在XML中定位节点的工具,同样适用于HTML文档节点定位
       只是一个工具,仅此而已,并不是Python中的模块

lxml解析库  +   xpath表达式

from lxml import etree

eobj = etree.HTML(html)
div_list = eobj.xpath('//div')
for div in div_list:
     item = {}
     item['title'] = div.xpath('./p/text()')
     item['href'] = div.xpath('./p/a/@href')



免费的代理IP：可用率很低、速度很慢












